{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e1cad271-e936-40f3-9f13-72798f01e16f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "This notebook is meant to:\n",
    "\n",
    "1. Test the input of deltatable information scraped from volume of pdfs.\n",
    "\n",
    "\n",
    "2. Use Chroma DB to create a local vector database from the document chunks.\n",
    "\n",
    "\n",
    "3. Use Hugging Face (all-MiniLM-L6-v2) to embed the text and LangChain to pass queries to LLaMA (Llama-2-7b-chat-hf) for response generation.\n",
    "\n",
    "\n",
    "4. Return a grounded answer by combining the retrieved document context with LLaMA's language capabilities.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "55caded3-a4c4-4b20-99a7-f171b75d5b66",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%pip install -U transformers accelerate bitsandbytes langchain chromadb sentence-transformers langchain-huggingface "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fcdbdbdd-b01c-437b-a74d-364c57a45703",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%pip install --upgrade accelerate\n",
    "%restart_python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "744c6b8f-7cb2-46d4-955f-480a4d22bb36",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from langchain.llms.base import LLM\n",
    "from langchain.schema import Document\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from langchain.vectorstores import Chroma\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2faba3b5-6a4c-478c-bdb7-55e164b36f6e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Load Document Chunks from Delta Table\n",
    "df = spark.sql(\"SELECT id, text FROM hive_metastore.sr_test.docs_text\").toPandas()\n",
    "\n",
    "documents = [\n",
    "    Document(page_content=row[\"text\"], metadata={\"id\": row[\"id\"]})\n",
    "    for _, row in df.iterrows()\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2234993b-e83b-4de0-8deb-61b2c94df12e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Setup Embeddings + Chroma Vector Store\n",
    "embedding_model = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "persist_dir = \"/tmp/chroma_llama_rag\"\n",
    "\n",
    "vector_db = Chroma.from_documents(documents=documents, embedding=embedding_model, persist_directory=persist_dir)\n",
    "retriever = vector_db.as_retriever(search_type=\"similarity\", search_kwargs={\"k\": 5})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "73e0966c-fc97-400a-9b36-827731936fc3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# STEP 5: Load LLaMA 2 from Hugging Face\n",
    "model_name = \"meta-llama/Llama-2-7b-chat-hf\"\n",
    "token = \"\" # Replace with your real token\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, token=token)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    torch_dtype=torch.float16,\n",
    "    device_map=\"auto\",\n",
    "    token=token\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "390eda37-66cb-4e54-8ed4-ae3da5da770e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# STEP 6: LangChain LLM Wrapper\n",
    "class Llama2LLM(LLM):\n",
    "    def _call(self, prompt: str, stop=None) -> str:\n",
    "        inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
    "        with torch.inference_mode():\n",
    "            outputs = model.generate(\n",
    "                **inputs,\n",
    "                do_sample=True,\n",
    "                temperature=0.7,\n",
    "                top_p=0.9,\n",
    "                max_new_tokens=500\n",
    "            )\n",
    "        return tokenizer.decode(outputs[0], skip_special_tokens=True).strip()\n",
    "\n",
    "    def invoke(self, input: str, config=None) -> str:\n",
    "        return self._call(input)\n",
    "\n",
    "    @property\n",
    "    def _llm_type(self):\n",
    "        return \"llama-2\"\n",
    "\n",
    "llm = Llama2LLM()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0e74ca90-07c7-440e-85a9-30c24ee45fab",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Prompt Template\n",
    "def build_llama_prompt(question, docs, max_chars=4000):\n",
    "    context = \"\\n\\n\".join([doc.page_content for doc in docs])[:max_chars]\n",
    "    return f\"\"\"<s>[INST] <<SYS>>\n",
    "You are a helpful grants AI assistant. Use the context below to answer the question clearly and concisely. Only answer questions that pertain to grants within the document reviewed.\n",
    "<</SYS>>\n",
    "\n",
    "Context:\n",
    "{context}\n",
    "\n",
    "Question:\n",
    "{question}\n",
    "[/INST]\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5af8e147-cebf-42ab-a394-27e878e3f2c3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# STEP 8: Ask a Question\n",
    "query = \"Give me 5 different grant projects with a 3 sentence summary of each.\"\n",
    "\n",
    "relevant_docs = retriever.invoke(query)\n",
    "prompt = build_llama_prompt(query, relevant_docs)\n",
    "response = llm.invoke(prompt)\n",
    "\n",
    "print(\"=== Prompt Sent ===\")\n",
    "print(prompt)\n",
    "print(\"\\n=== Response ===\")\n",
    "print(response)"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "3 Rag Based Chatbot Delta Table Input",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}